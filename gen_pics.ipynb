{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nerf_pl\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "\n",
    "import metrics\n",
    "\n",
    "from datasets import dataset_dict\n",
    "from datasets.llff import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "img_wh = (200, 200)\n",
    "\n",
    "\n",
    "dataset = dataset_dict['blender'] \\\n",
    "          ('./data/nerf_synthetic/mug/', 'test',\n",
    "           img_wh=img_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_embedding_xyz = Embedding(3, 10)\n",
    "hand_embedding_dir = Embedding(3, 4)\n",
    "object_embedding_xyz = Embedding(3, 10)\n",
    "object_embedding_dir = Embedding(3, 4)\n",
    "\n",
    "hand_nerf_coarse = NeRF()\n",
    "hand_nerf_fine = NeRF()\n",
    "object_nerf_coarse = NeRF()\n",
    "object_nerf_fine = NeRF()\n",
    "\n",
    "hand_ckpt_path = './ckpts/hand_flat/epoch=7.ckpt'\n",
    "object_ckpt_path = './ckpts/mug/epoch=7.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "load_ckpt(hand_nerf_coarse, hand_ckpt_path, model_name='nerf_coarse')\n",
    "load_ckpt(hand_nerf_fine, hand_ckpt_path, model_name='nerf_fine')\n",
    "load_ckpt(object_nerf_coarse, object_ckpt_path, model_name='nerf_coarse')\n",
    "load_ckpt(object_nerf_fine, object_ckpt_path, model_name='nerf_fine')\n",
    "\n",
    "hand_nerf_coarse.cuda().eval()\n",
    "hand_nerf_fine.cuda().eval()\n",
    "object_nerf_coarse.cuda().eval()\n",
    "object_nerf_fine.cuda().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.render_blend_mesh import render_rays_blend\n",
    "hand_models = [hand_nerf_coarse, hand_nerf_fine]\n",
    "hand_embeddings = [hand_embedding_xyz, hand_embedding_dir]\n",
    "object_models = [object_nerf_coarse, object_nerf_fine]\n",
    "object_embeddings = [object_embedding_xyz, object_embedding_dir]\n",
    "\n",
    "N_samples = 64\n",
    "N_importance = 64\n",
    "use_disp = False\n",
    "chunk = 1024*32*4\n",
    "\n",
    "@torch.no_grad()\n",
    "def f_trans(rays, poses = None, mano_layer = None, global_translation = None):\n",
    "    \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
    "    B = rays.shape[0]\n",
    "    results = defaultdict(list)\n",
    "    for i in range(0, B, chunk):\n",
    "        rendered_ray_chunks = \\\n",
    "            render_rays_blend(hand_models,\n",
    "                        hand_embeddings,\n",
    "                        object_models,\n",
    "                        object_embeddings,\n",
    "                        rays[i:i+chunk],\n",
    "                        N_samples,\n",
    "                        use_disp,\n",
    "                        0,\n",
    "                        0,\n",
    "                        N_importance,\n",
    "                        chunk,\n",
    "                        dataset.white_back,\n",
    "                        test_time=True,\n",
    "                        poses=poses,\n",
    "                        mano_layer=mano_layer,\n",
    "                        global_translation=global_translation)\n",
    "\n",
    "        for k, v in rendered_ray_chunks.items():\n",
    "            results[k] += [v]\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = torch.cat(v, 0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\WorkSpace\\nerf_pl\\mano\\manolayer.py:67: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  torch.Tensor(smpl_data['betas'].r).unsqueeze(0))\n"
     ]
    }
   ],
   "source": [
    "from manopth.manolayer import ManoLayer\n",
    "\n",
    "sample = dataset[0] # 18\n",
    "rays = sample['rays'].cuda()\n",
    "\n",
    "ncomps = 45\n",
    "\n",
    "import json\n",
    "\n",
    "result_dict = {}\n",
    "with open('./param.json', 'r') as f:\n",
    "    result_dict = json.load(f)\n",
    "\n",
    "poses_final = [result_dict['rot'] + result_dict['thetas']]\n",
    "poses_final = torch.tensor(poses_final)\n",
    "poses_init = torch.zeros_like(poses_final)\n",
    "global_translation_final = torch.tensor([result_dict['trans']]) * 12\n",
    "global_translation_final = global_translation_final.cuda()\n",
    "global_translation_init = torch.zeros_like(global_translation_final)\n",
    "\n",
    "mano_layer = ManoLayer(mano_root='./mano/models', use_pca=False, ncomps=ncomps, flat_hand_mean=True)\n",
    "shapes = torch.zeros(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [21:14<00:00, 15.73s/it]\n",
      "100%|██████████| 40/40 [10:23<00:00, 15.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "total = 80\n",
    "\n",
    "for i in tqdm(range(total + 1)):\n",
    "    poses = poses_init + (poses_final - poses_init) * (i / total)\n",
    "    global_translation = global_translation_init + (global_translation_final - global_translation_init) * (i / total)\n",
    "    results = f_trans(rays, poses, mano_layer, global_translation=global_translation)\n",
    "    torch.cuda.synchronize()\n",
    "    img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "    img_pred_ = (img_pred*255).astype(np.uint8)\n",
    "    img_path = f'./figs/grasp_mug_4/{i}.png'\n",
    "    imageio.imwrite(img_path, img_pred_)\n",
    "\n",
    "for i in tqdm(range(40)):\n",
    "    poses = poses_init + (poses_final - poses_init) * ((i + 1 + total) / total)\n",
    "    global_translation = global_translation_init + (global_translation_final - global_translation_init) * ((i + 1 + total) / total)\n",
    "    results = f_trans(rays, poses, mano_layer, global_translation=global_translation)\n",
    "    torch.cuda.synchronize()\n",
    "    img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "    img_pred_ = (img_pred*255).astype(np.uint8)\n",
    "    img_path = f'./figs/grasp_mug_4/{i + total + 1}.png'\n",
    "    imageio.imwrite(img_path, img_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:20<00:00, 14.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "total = 50\n",
    "\n",
    "for i in tqdm(range(total)):\n",
    "    poses = poses_init + torch.randn_like(poses_init) * 0.1\n",
    "    global_translation = global_translation_init + torch.randn_like(global_translation_init) * 0.1\n",
    "    results = f_trans(rays, poses, mano_layer, global_translation=global_translation)\n",
    "    torch.cuda.synchronize()\n",
    "    img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "    img_pred_ = (img_pred*255).astype(np.uint8)\n",
    "    img_path = f'./figs/grasp_mug_3/{i}.png'\n",
    "    imageio.imwrite(img_path, img_pred_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
